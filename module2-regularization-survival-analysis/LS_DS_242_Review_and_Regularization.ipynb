{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lambda School Data Science — Practicing & Understanding Predictive Modeling_\n",
    "\n",
    "# Review & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with example solution for yesterday's assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll modify a project from Python Data Science Handbook by Jake VanderPlas\n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Example:-Predicting-Bicycle-Traffic\n",
    "    \n",
    "# Predicting Bicycle Traffic\n",
    "\n",
    "# As an example, let's take a look at whether we can predict the number of \n",
    "# bicycle trips across Seattle's Fremont Bridge based on weather, season, \n",
    "# and other factors.\n",
    "\n",
    "# We will join the bike data with another dataset, and try to determine the \n",
    "# extent to which weather and seasonal factors—temperature, precipitation, \n",
    "# and daylight hours—affect the volume of bicycle traffic through this corridor. \n",
    "# Fortunately, the NOAA makes available their daily weather station data \n",
    "# (I used station ID USW00024233) and we can easily use Pandas to join \n",
    "# the two data sources.\n",
    "\n",
    "def load(): \n",
    "    fremont_bridge = 'https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD'\n",
    "    \n",
    "    bicycle_weather = 'https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/data/BicycleWeather.csv'\n",
    "\n",
    "    counts = pd.read_csv(fremont_bridge, index_col='Date', parse_dates=True, \n",
    "                         infer_datetime_format=True)\n",
    "\n",
    "    weather = pd.read_csv(bicycle_weather, index_col='DATE', parse_dates=True, \n",
    "                          infer_datetime_format=True)\n",
    "\n",
    "    daily = counts.resample('d').sum()\n",
    "    daily['Total'] = daily.sum(axis=1)\n",
    "    daily = daily[['Total']] # remove other columns\n",
    "\n",
    "    weather_columns = ['PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN', 'AWND']\n",
    "    daily = daily.join(weather[weather_columns], how='inner')\n",
    "    \n",
    "    # Make a feature for yesterday's total\n",
    "    daily['Total_yesterday'] = daily.Total.shift(1)\n",
    "    daily = daily.drop(index=daily.index[0])\n",
    "    \n",
    "    return daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(daily):\n",
    "    # Hold out an \"out-of-time\" test set, from the last 100 days of data\n",
    "    \n",
    "    train = daily[:-100]\n",
    "    test = daily[-100:]\n",
    "    \n",
    "    X_train = train.drop(columns='Total')\n",
    "    y_train = train.Total\n",
    "\n",
    "    X_test  = test.drop(columns='Total')\n",
    "    y_test  = test.Total\n",
    "    \"\"\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jake_wrangle(X):  \n",
    "    X = X.copy()\n",
    "\n",
    "    # patterns of use generally vary from day to day; \n",
    "    # let's add binary columns that indicate the day of the week:\n",
    "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    for i, day in enumerate(days):\n",
    "        X[day] = (X.index.dayofweek == i).astype(float)\n",
    "\n",
    "\n",
    "    # we might expect riders to behave differently on holidays; \n",
    "    # let's add an indicator of this as well:\n",
    "    from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holidays = cal.holidays('2012', '2016')\n",
    "    X = X.join(pd.Series(1, index=holidays, name='holiday'))\n",
    "    X['holiday'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "    # We also might suspect that the hours of daylight would affect \n",
    "    # how many people ride; let's use the standard astronomical calculation \n",
    "    # to add this information:\n",
    "    def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "        \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
    "        days = (date - pd.datetime(2000, 12, 21)).days\n",
    "        m = (1. - np.tan(np.radians(latitude))\n",
    "             * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "        return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
    "\n",
    "    X['daylight_hrs'] = list(map(hours_of_daylight, X.index))\n",
    "\n",
    "\n",
    "    # temperatures are in 1/10 deg C; convert to C\n",
    "    X['TMIN'] /= 10\n",
    "    X['TMAX'] /= 10\n",
    "\n",
    "    # We can also calcuate the average temperature.\n",
    "    X['Temp (C)'] = 0.5 * (X['TMIN'] + X['TMAX'])\n",
    "\n",
    "\n",
    "    # precip is in 1/10 mm; convert to inches\n",
    "    X['PRCP'] /= 254\n",
    "\n",
    "    # In addition to the inches of precipitation, let's add a flag that \n",
    "    # indicates whether a day is dry (has zero precipitation):\n",
    "    X['dry day'] = (X['PRCP'] == 0).astype(int)\n",
    "\n",
    "\n",
    "    # Let's add a counter that increases from day 1, and measures how many \n",
    "    # years have passed. This will let us measure any observed annual increase \n",
    "    # or decrease in daily crossings:\n",
    "    X['annual'] = (X.index - X.index[0]).days / 365.\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and join data into a dataframe\n",
    "data = load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `SNOW` and `AWND` have minimums of `-9999` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data and engineer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "    X = X.replace(-9999, 0)\n",
    "    X = jake_wrangle(X)\n",
    "    \n",
    "    # DS1 DH\n",
    "    X['PRCP_yesterday'] = X.PRCP.shift(1).fillna(X.PRCP.mean())\n",
    "    X['Windchill'] = (((X['Temp (C)'] * (9/5) + 32) * .6215) + 34.74) - (35.75 * (X['AWND']** .16)) + (.4275 * (X['Temp (C)'])) * (X['AWND'] ** .16)\n",
    "    X['Rl_Cold'] = (((X['Temp (C)'] * (9/5) + 32) - X['Windchill']) -32) * (5/9)\n",
    "    X['TMIN_squared'] = X['TMIN'] **2\n",
    "    \n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    for i, month in enumerate(months):\n",
    "        X[month] = (X.index.month == i+1).astype(float)\n",
    "    \n",
    "    # DS3 JD\n",
    "    X['light_rain'] = (X['PRCP'] > 0) & (X['PRCP'] < 0.10)\n",
    "    X['moderate_rain'] = (X['PRCP'] >= 0.1) & (X['PRCP'] < 0.30)\n",
    "    X['heavy_rain'] = (X['PRCP'] >= 0.30)\n",
    "    X['weekend_day'] = (X['Sat'] == 1) | (X['Sun'] == 1)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = split(data)\n",
    "\n",
    "# Do the same wrangling to X_train and X_test\n",
    "X_train = wrangle(X_train)\n",
    "X_test  = wrangle(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV review / SPRINT CHALLENGE PRACTICE\n",
    "- https://twitter.com/jakevdp/status/648593367786323968\n",
    "- https://www.google.com/search?q=site%3Ascikit-learn.org+RandomizedSearchCV\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "  - https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n",
    "- https://www.google.com/search?q=XGBRegressor\n",
    "  - https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "- https://www.google.com/search?q=xgboost+parameter+tuning\n",
    "  - https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "- https://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the last step\n",
    "\n",
    "Yesterday's assingment said,\n",
    "\n",
    "At the end of the day, take the last step in the \"universal workflow of machine learning\" — \"You can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\"\n",
    "\n",
    "See the [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) documentation for the `refit` parameter, `best_estimator_` attribute, and `predict` method:\n",
    "\n",
    "> **refit : boolean, or string, default=True**\n",
    "\n",
    "> Refit an estimator using the best found parameters on the whole dataset.\n",
    "\n",
    "> The refitted estimator is made available at the `best_estimator_` attribute and permits using `predict` directly on this `GridSearchCV` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few new things!\n",
    "- Pipeline + RandomizedSearchCV\n",
    "- SelectKBest for feature selection, in a pipeline\n",
    "- Ridge Regression, a linear model with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows which features were selected\n",
    "selector = search.best_estimator_.named_steps['selectkbest']\n",
    "all_names = X_train.columns\n",
    "selected_mask = selector.get_support()\n",
    "selected_names = all_names[selected_mask]\n",
    "unselected_names = all_names[~selected_mask]\n",
    "\n",
    "print('Features selected:')\n",
    "for name in selected_names:\n",
    "    print(name)\n",
    "\n",
    "print('\\n', 'Features not selected:')\n",
    "for name in unselected_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with Linear Models\n",
    "\n",
    "- https://www.google.com/search?q=site%3Ascikit-learn.org+regularization+%22linear+models%22\n",
    "  - https://scikit-learn.org/stable/modules/linear_model.html\n",
    "  - https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression\n",
    "  - https://scikit-learn.org/stable/modules/linear_model.html#lasso\n",
    "  - https://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter\n",
    "  - https://scikit-learn.org/stable/modules/linear_model.html#using-cross-validation\n",
    "    \n",
    "> The [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer non-zero coefficients, effectively reducing the number of features upon which the given solution is dependent. \n",
    "  \n",
    "> The `alpha` parameter controls the degree of sparsity of the estimated coefficients.\n",
    "  \n",
    "> scikit-learn exposes objects that set the Lasso `alpha` parameter by cross-validation: [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "def plot_lasso_coefs(power=-2):\n",
    "    # Make pipeline with StandardScaler and Lasso Linear Model\n",
    "    alpha = 10**power\n",
    "    print(f'Lasso Alpha: {alpha}')\n",
    "    pipe = make_pipeline(StandardScaler(), Lasso(alpha=alpha, max_iter=100000))\n",
    "    \n",
    "    # Get cross-validation scores\n",
    "    maes = -cross_val_score(pipe, X_train, y_train, scoring='neg_mean_absolute_error', cv=3)\n",
    "    print('Cross-Validated Mean Absolute Error:', maes.mean())\n",
    "    \n",
    "    # Get coefficients\n",
    "    pipe.fit(X_train, y_train)\n",
    "    coefficients = pd.Series(pipe.named_steps['lasso'].coef_, X_train.columns)\n",
    "    \n",
    "    # Plot coefficients\n",
    "    plt.figure(figsize=(16,8))\n",
    "    coefficients.sort_values().plot.barh(color='grey')\n",
    "    plt.xlim(-550,550)\n",
    "\n",
    "interact(plot_lasso_coefs, power=(-2,2,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LassoCV(cv=3))\n",
    "pipe.fit(X_train, y_train)\n",
    "lasso = pipe.named_steps['lassocv']\n",
    "print('Lasso Linear Model, alpha value optimized with cross validation:', lasso.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = lasso.alphas_\n",
    "mses = lasso.mse_path_.mean(axis=1)\n",
    "rmses = np.sqrt(mses)\n",
    "plt.plot(alphas, rmses)\n",
    "plt.title('Lasso Linear Model')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, rmses)\n",
    "plt.title('Lasso Linear Model')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlim(1,10)\n",
    "plt.ylim(380,400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more about regularized linear models?\n",
    "- Aaron Gallant, [Ridge Regression](https://www.youtube.com/watch?v=XK5jkedy17w) (9 minute video)\n",
    "- Selecting good features, [Part 2](https://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/) and [Part 4](https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/)\n",
    "- [_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/), Chapters 3 & 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
